<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>YOLOv5 PT转Engine核心：RTX环境依赖配置全攻略（附避坑指南） | 我的 Markdown 文章</title>
    <meta name="description" content="用 VitePress 搭建的文章展示网站">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/assets/style.DFTx90Kk.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.BXNXXhu6.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.CWt1tcxf.js">
    <link rel="modulepreload" href="/assets/chunks/framework.CBTkueSR.js">
    <link rel="modulepreload" href="/assets/tutorials_first-tutorial.md.DuUH76rD.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0b0ada53></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0b0ada53>Skip to content</a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle has-sidebar" data-v-6aa21345 data-v-1168a8e4><a class="title" href="/" data-v-1168a8e4><!--[--><!--]--><!----><span data-v-1168a8e4>我的 Markdown 文章</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!----></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>首页</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-cf11d7a2><span class="text" data-v-cf11d7a2><!----><span data-v-cf11d7a2>教程</span><span class="vpi-chevron-down text-icon" data-v-cf11d7a2></span></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="../tutorials/first-tutorial.html" data-v-35975db6><!--[--><span data-v-35975db6>教程1</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="../tutorials/second-tutorial.html" data-v-35975db6><!--[--><span data-v-35975db6>教程2</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-cf11d7a2><span class="text" data-v-cf11d7a2><!----><span data-v-cf11d7a2>笔记</span><span class="vpi-chevron-down text-icon" data-v-cf11d7a2></span></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="../notes/study-note.html" data-v-35975db6><!--[--><span data-v-35975db6>学习笔记</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/second-article.html" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>第二篇文章</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/about.html" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>关于</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><!----><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-cf11d7a2><span class="vpi-more-horizontal icon" data-v-cf11d7a2></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><!----><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>Appearance</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><!----><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-a6f0e41e><span class="vpi-align-left menu-icon" data-v-a6f0e41e></span><span class="menu-text" data-v-a6f0e41e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-8a42e2b4><button data-v-8a42e2b4>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-5d98c3a5 data-v-319d5ca6><div class="curtain" data-v-319d5ca6></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-319d5ca6><span class="visually-hidden" id="sidebar-aria-label" data-v-319d5ca6> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0 has-active" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>教程列表</h2><!----></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/tutorials/first-tutorial.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>first-tutorial</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/tutorials/second-tutorial.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>second-tutorial</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-sidebar has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>On this page</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _tutorials_first-tutorial" data-v-39a288b8><div><h1 id="yolov5-pt转engine核心-rtx环境依赖配置全攻略-附避坑指南" tabindex="-1">YOLOv5 PT转Engine核心：RTX环境依赖配置全攻略（附避坑指南） <a class="header-anchor" href="#yolov5-pt转engine核心-rtx环境依赖配置全攻略-附避坑指南" aria-label="Permalink to &quot;YOLOv5 PT转Engine核心：RTX环境依赖配置全攻略（附避坑指南）&quot;">​</a></h1><p>在实时目标检测场景里，YOLOv5训练生成的PT权重虽便捷，但推理速度远不能发挥RTX显卡的性能——而TensorRT优化后的Engine格式，能让推理效率提升30%-50%。网上充斥着PT转Engine的完整流程教程，但<strong>没人系统讲过依赖配置的门道</strong>。我在Ubuntu 24.04上反复踩坑十余次才打通流程，发现依赖版本匹配、安装顺序、源配置这三大块正是卡壳重灾区。这篇就聚焦最关键的依赖配置，帮你绕开90%的启动失败问题。</p><h2 id="一、先厘清概念-别搞混tensorrtx与tensorrt" tabindex="-1">一、先厘清概念：别搞混TensorRTX与TensorRT <a class="header-anchor" href="#一、先厘清概念-别搞混tensorrtx与tensorrt" aria-label="Permalink to &quot;一、先厘清概念：别搞混TensorRTX与TensorRT&quot;">​</a></h2><ul><li><p><strong>TensorRT</strong>：NVIDIA官方推出的深度学习推理优化SDK，核心作用是对模型进行量化、剪枝等优化，生成高效的推理引擎（Engine文件），是“加速能力”的核心提供者。</p></li><li><p><strong>TensorRTX</strong>：wang-xinyu 开发的开源项目（<a href="https://github.com/wang-xinyu/tensorrtx" target="_blank" rel="noreferrer">官方仓库</a>），为YOLOv5等热门模型提供了适配TensorRT的代码实现，相当于“桥梁”——让我们能快速把YOLOv5的PT权重通过TensorRT转成Engine。</p></li><li><p><strong>关键逻辑</strong>：我们要配的依赖，本质是让“桥梁（TensorRTX）”能正常调用“加速核心（TensorRT）”，而这一切都依赖NVIDIA显卡的底层支持（驱动、CUDA、cuDNN）。</p></li></ul><h2 id="二、核心-rtx环境依赖配置全流程-附实测有效版本" tabindex="-1">二、核心：RTX环境依赖配置全流程（附实测有效版本） <a class="header-anchor" href="#二、核心-rtx环境依赖配置全流程-附实测有效版本" aria-label="Permalink to &quot;二、核心：RTX环境依赖配置全流程（附实测有效版本）&quot;">​</a></h2><p>先放我最终打通的<strong>版本匹配表</strong>（重中之重！版本不匹配会报各种玄学错误，别乱换）：</p><table tabindex="0"><thead><tr><th>组件</th><th>实测有效版本</th><th>作用说明</th></tr></thead><tbody><tr><td>操作系统</td><td>Ubuntu 24.04 LTS</td><td>稳定且对新显卡支持友好，建议用官方中国镜像源安装</td></tr><tr><td>NVIDIA显卡驱动</td><td>580系列（如580.xx.xx，选择非test、非server版本）</td><td>显卡底层驱动，需支持CUDA 12.0，选择非test、非server的稳定版本</td></tr><tr><td>CUDA</td><td>12.0（通过apt安装，对应cuda-12-0包）</td><td>显卡计算框架，TensorRT依赖其运行</td></tr><tr><td>cuDNN</td><td>8.9.2（for CUDA 12.0）</td><td>CUDA的深度学习加速库，提升TensorRT的卷积计算效率</td></tr><tr><td>TensorRT</td><td>8.6.1.6（TensorRT-8.6.1.6.Ubuntu-22.04.x86_64-gnu.cuda-12.0.tar.gz）</td><td>核心优化工具，负责生成Engine文件</td></tr><tr><td>Python环境</td><td>Miniconda 24.3.0 + Python 3.10</td><td>管理依赖包，避免系统环境污染</td></tr><tr><td>PyTorch</td><td>适配CUDA 12.0的版本（建议从PyTorch官网获取）</td><td>加载PT权重，配合TensorRTX执行转换</td></tr></tbody></table><h3 id="_2-1-第一步-配置系统源-避免后续安装卡壳" tabindex="-1">2.1 第一步：配置系统源（避免后续安装卡壳） <a class="header-anchor" href="#_2-1-第一步-配置系统源-避免后续安装卡壳" aria-label="Permalink to &quot;2.1 第一步：配置系统源（避免后续安装卡壳）&quot;">​</a></h3><p>Ubuntu默认源在国内速度慢，先换成清华源，同时配置conda和pip源，后续安装一路丝滑：</p><ol><li><p><strong>系统源替换</strong>：打开“软件和更新”，将“下载自”改为“位于中国的服务器”，刷新缓存后更新： <code>sudo apt update &amp;&amp; sudo apt upgrade -y</code></p></li><li><p><strong>conda源配置</strong>：安装Miniconda后，执行以下命令添加清华源：</p><pre><code> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/

 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/

 conda config --set show_channel_urls yes
</code></pre></li><li><p><strong>pip源配置</strong>：创建pip配置文件：</p><pre><code> mkdir -p ~/.config/pip

 echo &quot;[global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple&quot; &gt; ~/.config/pip/pip.conf
</code></pre></li></ol><h3 id="_2-2-第二步-安装nvidia显卡驱动-最容易忽略的前置" tabindex="-1">2.2 第二步：安装NVIDIA显卡驱动（最容易忽略的前置） <a class="header-anchor" href="#_2-2-第二步-安装nvidia显卡驱动-最容易忽略的前置" aria-label="Permalink to &quot;2.2 第二步：安装NVIDIA显卡驱动（最容易忽略的前置）&quot;">​</a></h3><p>驱动是基础，必须先装且版本适配CUDA！经过实测，Ubuntu自带的“附加驱动”能精准匹配系统内核，安装更便捷稳定，推荐优先使用此方式，重点选择“非test、非server”的最新稳定版本：</p><ol><li><p>卸载旧驱动（如果之前手动装过或有残留）：</p><pre><code> sudo apt purge nvidia-* -y

 # 执行后重启电脑

 sudo reboot
</code></pre></li><li><p>打开“附加驱动”：通过Ubuntu搜索栏输入“附加驱动”打开工具，等待系统自动扫描适配的显卡驱动</p></li><li><p>选择驱动版本：在扫描结果中，选择“非test（测试版）、非server（服务器版）”的最新稳定版本，例如我选择的“NVIDIA Corporation: NVIDIA GeForce RTX XXX”对应的580系列驱动</p></li><li><p>应用驱动：点击“应用更改”，系统会自动下载并安装所选驱动，过程中需输入密码授权，等待安装完成（约5-10分钟，视网络速度而定）</p></li><li><p>重启生效：安装完成后，点击“重启”按钮，或手动执行<code>sudo reboot</code>重启电脑</p></li><li><p>验证：重启后打开终端，执行<code>nvidia-smi</code>，若能显示显卡型号、驱动版本（如580.xx.xx）及支持的CUDA版本（如“CUDA Version: 12.0”），则驱动安装成功</p></li></ol><h3 id="_2-3-第三步-安装cuda-12-0-严格匹配版本" tabindex="-1">2.3 第三步：安装CUDA 12.0（严格匹配版本） <a class="header-anchor" href="#_2-3-第三步-安装cuda-12-0-严格匹配版本" aria-label="Permalink to &quot;2.3 第三步：安装CUDA 12.0（严格匹配版本）&quot;">​</a></h3><p>CUDA版本必须和TensorRT、cuDNN对应，这里选12.0：</p><ol><li><p>从<a href="https://developer.nvidia.com/cuda-12.0.1-download-archive" target="_blank" rel="noreferrer">CUDA官网</a>下载12.0版本（选择Linux→x86_64→Ubuntu→22.04→runfile）</p></li><li><p>执行安装：严格按照官网步骤逐行代码执行即可。。。</p></li><li><p>。。。</p></li></ol><p>or 最简单安装：sudo apt install cuda</p><h3 id="_2-4-第四步-安装cudnn-8-9-2-cuda的-加速器" tabindex="-1">2.4 第四步：安装cuDNN 8.9.2（CUDA的“加速器”） <a class="header-anchor" href="#_2-4-第四步-安装cudnn-8-9-2-cuda的-加速器" aria-label="Permalink to &quot;2.4 第四步：安装cuDNN 8.9.2（CUDA的“加速器”）&quot;">​</a></h3><p>cuDNN是为深度学习优化的库，必须选对应CUDA 12.0的版本：</p><ol><li><p>从<a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noreferrer">cuDNN官网</a>下载“cuDNN Library for Linux x86_64”（版本8.9.7，for CUDA 12.x）</p></li><li><p>执行安装：严格按照官网步骤逐行代码执行即可。。。</p></li><li><p>。。。</p></li></ol><h3 id="_2-5-第五步-安装tensorrt-8-6-1-6-核心转换工具" tabindex="-1">2.5 第五步：安装TensorRT 8.6.1.6（核心转换工具） <a class="header-anchor" href="#_2-5-第五步-安装tensorrt-8-6-1-6-核心转换工具" aria-label="Permalink to &quot;2.5 第五步：安装TensorRT 8.6.1.6（核心转换工具）&quot;">​</a></h3><p>TensorRT是生成Engine的关键，这里用tar包安装（比deb包更灵活）：</p><ol><li><p>从<a href="https://developer.nvidia.com/nvidia-tensorrt-8x-download" target="_blank" rel="noreferrer">TensorRT官网</a>下载对应版本（TensorRT-8.x）</p></li><li><p>解压到指定目录：<code>tar -xzvf TensorRT-8.6.1.6.Ubuntu-22.04.x86_64-gnu.cuda-12.0.tar.gz -C ~/tools/</code></p></li><li><p>配置环境变量：打开<code>~/.bashrc</code>，添加：</p><pre><code> export TENSORRT_DIR=~/tools/TensorRT-8.6.1.6

 export PATH=$TENSORRT_DIR/bin:$PATH

 export LD_LIBRARY_PATH=$TENSORRT_DIR/lib:$LD_LIBRARY_PATH
</code></pre></li><li><p>生效环境变量：<code>source ~/.bashrc</code></p></li><li><p>安装Python绑定：进入TensorRT的python目录，根据当前Python版本选择对应whl文件安装（以Python 3.10为例）：<code>cd ~/tools/TensorRT-8.6.1.6/python &amp;&amp; pip install tensorrt-8.6.1.6-cp310-none-linux_x86_64.whl</code></p></li><li><p>验证（可选）：Python中执行<code>import tensorrt as trt; print(trt.__version__)</code>，能输出8.6.1.6</p></li></ol><h3 id="_2-6-第六步-配置python环境-加载pt权重用" tabindex="-1">2.6 第六步：配置Python环境（加载PT权重用） <a class="header-anchor" href="#_2-6-第六步-配置python环境-加载pt权重用" aria-label="Permalink to &quot;2.6 第六步：配置Python环境（加载PT权重用）&quot;">​</a></h3><p>用Miniconda创建独立环境，避免依赖冲突：</p><ol><li><p>创建环境：<code>conda create -n yolov5-trt python=3.10 -y</code></p></li><li><p>激活环境：<code>conda activate yolov5-trt</code></p></li><li><p><strong>安装PyTorch</strong>：建议前往<a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noreferrer">PyTorch官网</a>，根据实际环境（CUDA 12.0、Python 3.10、pip）选择对应配置，复制官网提供的pip安装命令执行（官网命令会适配最新兼容版本，避免手动指定版本可能出现的适配问题）。</p></li><li><p><strong>安装YOLOv5依赖</strong>：推荐通过YOLOv5官方的requirements.txt文件安装，确保依赖版本与YOLOv5适配。先克隆YOLOv5项目获取配置文件（若已克隆可跳过）：<code>git clone https://github.com/ultralytics/yolov5.git</code>，进入项目目录后执行：<code>pip install -r requirements.txt</code>（若需指定版本，可编辑requirements.txt后再安装）。</p></li></ol><h2 id="三、避坑指南-我踩过的5个致命错误-附解决方案" tabindex="-1">三、避坑指南：我踩过的5个致命错误（附解决方案） <a class="header-anchor" href="#三、避坑指南-我踩过的5个致命错误-附解决方案" aria-label="Permalink to &quot;三、避坑指南：我踩过的5个致命错误（附解决方案）&quot;">​</a></h2><p>这些问题网上搜不到明确答案，都是我实测踩坑总结的，遇到直接照解：</p><p><strong>坑1：通过apt安装CUDA时未清理旧驱动，导致冲突报错</strong></p><p>解决方案：apt安装CUDA时默认不会强制覆盖旧驱动，需先彻底清理残留：执行<code>sudo apt purge nvidia-* cuda-* -y</code>，重启后重新通过“附加驱动”装显卡驱动，再按步骤安装CUDA即可。</p><p><strong>坑2：Import tensorrt时报“libnvinfer.so.8: cannot open shared object file”</strong></p><p>解决方案：不是没装TensorRT，而是环境变量没生效！执行<code>source ~/.bashrc</code>重新加载，若还是报错，检查TENSORRT_DIR路径是否正确（确保和实际解压路径一致）。</p><p><strong>坑3：安装cuDNN后，执行TensorRT示例报错“CUDNN_STATUS_VERSION_MISMATCH”</strong></p><p>解决方案：cuDNN版本和CUDA不匹配！比如用了CUDA 12.0却装了for CUDA 11.8的cuDNN，重新下载对应版本的cuDNN（参考2.4步骤）。</p><p><strong>坑4：nvidia-smi显示CUDA Version 12.0，但nvcc -V显示11.7</strong></p><p>原因说明：</p><ul><li><code>nvidia-smi</code>显示的是<strong>显卡驱动支持的最高CUDA版本</strong>（此处12.0代表驱动可兼容≤12.0的CUDA）；</li><li><code>nvcc -V</code>显示的是<strong>当前系统实际生效的CUDA版本</strong>（此处11.7说明环境变量指向了旧版本）。</li></ul><p>两者不一致的核心问题是：系统中同时安装了多个CUDA版本（如11.7和12.0），但环境变量仍指向旧版本11.7，导致编译时调用的是旧版本。</p><p>解决方案：</p><ol><li>打开环境变量配置文件：<code>vim ~/.bashrc</code>（若不熟悉vim，可改用<code>gedit ~/.bashrc</code>图形化编辑）；</li><li>找到与CUDA相关的路径设置（如<code>export PATH=/usr/local/cuda-11.7/bin:$PATH</code>和<code>export LD_LIBRARY_PATH=/usr/local/cuda-11.7/lib64:$LD_LIBRARY_PATH</code>）</li></ol><ul><li><p>删除旧版本（11.7）的配置，仅保留目标版本（12.0）的路径</p></li><li><p>例如： <code>export PATH=/usr/local/cuda-12.0/bin:$PATH </code> <code>export LD_LIBRARY_PATH=/usr/local/cuda-12.0/lib64:$LD_LIBRARY_PATH</code></p></li></ul><ol start="3"><li>保存文件后，执行<code>source ~/.bashrc</code>使配置生效，建议重启终端验证：</li></ol><ul><li>再次运行<code>nvcc -V</code>，确认显示版本为12.0；</li><li><code>nvidia-smi</code>显示的驱动支持版本无需修改，保持与实际安装的CUDA版本兼容即可（≤12.0）。</li></ul><p><strong>坑5：Pip安装torch时速度极慢，甚至超时</strong></p><p>原因说明： 安装PyTorch（尤其是GPU版本）时速度慢的核心原因通常有两点：</p><ol><li>直接使用官方默认源（pypi.org）时，服务器位于境外，国内网络访问可能存在延迟或带宽限制；</li><li>若配置了第三方镜像源（如清华源），可能因源地址错误、配置文件路径不正确（如非<code>~/.config/pip/pip.conf</code>或<code>~/.pip/pip.conf</code>），或镜像源同步延迟导致无法生效。</li></ol><p>需注意：PyTorch官网提供的安装命令（含<code>https://download.pytorch.org/whl/</code>源）是针对GPU版本的官方推荐方式，该源本身无访问限制，但国内网络环境可能仍存在连接不稳定的问题。</p><p>解决方案：</p><ol><li><strong>优先使用官网命令+国内镜像加速</strong>：</li></ol><ul><li>例外：从PyTorch官网（<a href="https://pytorch.org/%EF%BC%89%E8%8E%B7%E5%8F%96%E5%AF%B9%E5%BA%94CUDA" target="_blank" rel="noreferrer">https://pytorch.org/）获取对应CUDA</a> 12.0版本的安装命令（如<code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121</code>），</li></ul><ol start="2"><li><strong>检查第三方镜像源配置</strong>：</li></ol><ul><li>若需长期使用镜像源，确认<code>pip.conf</code>路径正确（Linux通常为<code>~/.config/pip/pip.conf</code>或<code>~/.pip/pip.conf</code>），文件内容示例： <code>[global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple [install] </code> <code>trusted-host = pypi.tuna.tsinghua.edu.cn</code> 保存后重新执行安装命令。</li></ul><ol start="3"><li><strong>临时解决网络问题</strong>：</li></ol><ul><li>若上述方法仍超时，可尝试切换网络环境（如使用有线连接），或通过代理工具提升境外连接稳定性。</li></ul><h2 id="四、总结与后续-依赖配好-转换就成功了80" tabindex="-1">四、总结与后续：依赖配好，转换就成功了80% <a class="header-anchor" href="#四、总结与后续-依赖配好-转换就成功了80" aria-label="Permalink to &quot;四、总结与后续：依赖配好，转换就成功了80%&quot;">​</a></h2><p>很多人卡在PT转Engine的第一步，不是代码有问题，而是依赖配置没到位——毕竟NVIDIA的这套生态对版本匹配要求极高，差一个小版本都可能报错。我上面给的版本表和步骤都是实测跑通的，只要严格照做，基本能绕开所有基础坑。</p><p>配好依赖后，后续的PT转Engine核心步骤可参考官方教程（附录已提供权威指引）。如果执行中遇到其他问题，欢迎评论区留言，我会把你的问题补充到避坑指南里，帮助更多人少走弯路～</p><h2 id="附录-pt转engine核心流程指引" tabindex="-1">附录：PT转Engine核心流程指引 <a class="header-anchor" href="#附录-pt转engine核心流程指引" aria-label="Permalink to &quot;附录：PT转Engine核心流程指引&quot;">​</a></h2><p>完成前文依赖配置后，PT转Engine的核心流程可直接参考TensorRTX官方提供的YOLOv5专属教程，该教程会根据YOLOv5版本动态更新适配步骤，比通用流程更精准权威：</p><p>官方教程链接：<a href="https://github.com/wang-xinyu/tensorrtx/tree/master/yolov5" target="_blank" rel="noreferrer">https://github.com/wang-xinyu/tensorrtx/tree/master/yolov5</a></p><p>官方教程核心优势：① 明确标注各YOLOv5版本（v5/v6/v7等）对应的分支切换方法；② 提供权重转换、编译推理的完整命令及常见问题解答；③ 实时更新适配最新系统环境的操作细节。</p><h2 id="参考资料" tabindex="-1">参考资料 <a class="header-anchor" href="#参考资料" aria-label="Permalink to &quot;参考资料&quot;">​</a></h2><ul><li><p>[1] TensorRTX官方仓库. <a href="https://github.com/wang-xinyu/tensorrtx" target="_blank" rel="noreferrer">https://github.com/wang-xinyu/tensorrtx</a></p></li><li><p>[2] NVIDIA CUDA 12.0官方文档. <a href="https://docs.nvidia.com/cuda/12.0/index.html" target="_blank" rel="noreferrer">https://docs.nvidia.com/cuda/12.0/index.html</a></p></li><li><p>[3] YOLOv5配置与训练笔记. <a href="https://www.cnblogs.com/tokepson/p/18817469" target="_blank" rel="noreferrer">https://www.cnblogs.com/tokepson/p/18817469</a></p></li></ul><blockquote><p>（注：文档部分内容可能由 AI 生成，有错误请联系作者修改）</p></blockquote></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><!----></div><div class="pager" data-v-e257564d><a class="VPLink link pager-link next" href="/tutorials/second-tutorial.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Next page</span><span class="title" data-v-e257564d>second-tutorial</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-5d98c3a5 data-v-e315a0ad><div class="container" data-v-e315a0ad><p class="message" data-v-e315a0ad>部署于 GitHub Pages | 欢迎访问</p><!----></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api-examples.md\":\"vDdmIjJt\",\"index.md\":\"DY6FWL-G\",\"markdown-examples.md\":\"BFs8NFVI\",\"notes_study-note.md\":\"B3_z4LCU\",\"tutorials_first-tutorial.md\":\"DuUH76rD\",\"tutorials_second-tutorial.md\":\"C4GlaJN3\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"我的 Markdown 文章\",\"description\":\"用 VitePress 搭建的文章展示网站\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"教程\",\"items\":[{\"text\":\"教程1\",\"link\":\"../tutorials/first-tutorial\"},{\"text\":\"教程2\",\"link\":\"../tutorials/second-tutorial\"}]},{\"text\":\"笔记\",\"items\":[{\"text\":\"学习笔记\",\"link\":\"../notes/study-note\"}]},{\"text\":\"第二篇文章\",\"link\":\"/second-article\"},{\"text\":\"关于\",\"link\":\"/about\"}],\"sidebar\":{\"/tutorials/\":[{\"text\":\"教程列表\",\"items\":[{\"text\":\"first-tutorial\",\"link\":\"/tutorials/first-tutorial\"},{\"text\":\"second-tutorial\",\"link\":\"/tutorials/second-tutorial\"}]}],\"/notes/\":[{\"text\":\"笔记列表\",\"items\":[{\"text\":\"学习笔记\",\"link\":\"../notes/study-note\"}]}],\"/\":[{\"text\":\"首页文章\",\"items\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"第二篇文章\",\"link\":\"/second-article\"}]}]},\"footer\":{\"message\":\"部署于 GitHub Pages | 欢迎访问\"}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>